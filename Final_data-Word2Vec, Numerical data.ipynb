{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ABC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ABC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import calendar\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import ShuffleSplit , StratifiedKFold , RepeatedStratifiedKFold ,train_test_split\n",
    "from sklearn.model_selection import cross_val_score , cross_validate ,cross_val_predict\n",
    "from sklearn.metrics import plot_roc_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve, precision_recall_curve , classification_report\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import itertools\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(sns.husl_palette(9, s=0.7 ))\n",
    "sns.set_style(style='white')\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.impute import KNNImputer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "#imports related to modeling\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "data = pd.read_csv(r'C:\\Users\\ABC\\Downloads\\data_ted_talks.csv')\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4005 entries, 0 to 4004\n",
      "Data columns (total 19 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   talk_id         4005 non-null   int64  \n",
      " 1   title           4005 non-null   object \n",
      " 2   speaker_1       4005 non-null   object \n",
      " 3   all_speakers    4001 non-null   object \n",
      " 4   occupations     3483 non-null   object \n",
      " 5   about_speakers  3502 non-null   object \n",
      " 6   views           4005 non-null   int64  \n",
      " 7   recorded_date   4004 non-null   object \n",
      " 8   published_date  4005 non-null   object \n",
      " 9   event           4005 non-null   object \n",
      " 10  native_lang     4005 non-null   object \n",
      " 11  available_lang  4005 non-null   object \n",
      " 12  comments        3350 non-null   float64\n",
      " 13  duration        4005 non-null   int64  \n",
      " 14  topics          4005 non-null   object \n",
      " 15  related_talks   4005 non-null   object \n",
      " 16  url             4005 non-null   object \n",
      " 17  description     4005 non-null   object \n",
      " 18  transcript      4005 non-null   object \n",
      "dtypes: float64(1), int64(3), object(15)\n",
      "memory usage: 594.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['all_speakers'].fillna(\"{0:['NaN']}\",inplace=True,axis=0)\n",
    "df['all_speakers'] = df['all_speakers'].apply(ast.literal_eval)\n",
    "\n",
    "df['occupations'].fillna(\"{0:['NaN']}\",inplace=True,axis=0)\n",
    "df['occupations'] = df['occupations'].apply(ast.literal_eval)\n",
    "\n",
    "df['about_speakers'].fillna(\"{0:['NaN']}\",inplace=True,axis=0)\n",
    "df['about_speakers'] = df['about_speakers'].apply(ast.literal_eval)\n",
    "\n",
    "df['published_date'] = df['published_date'].apply(lambda x: datetime.datetime.strptime((x),\"%Y-%m-%d\"))\n",
    "\n",
    "df.drop(df[df['recorded_date'].isna()].index[0], inplace = True)\n",
    "df['recorded_date'] = df['recorded_date'].apply(lambda x: datetime.datetime.strptime((x),\"%Y-%m-%d\"))\n",
    "\n",
    "df['total_days_since_published'] = (df['published_date'].max() - df['published_date']).apply(lambda x : x.days )\n",
    "\n",
    "month_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "day_order   = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "df['month'] = pd.DatetimeIndex(df['published_date']).month\n",
    "df['month'] = df['month'].apply(lambda x: calendar.month_abbr[x])\n",
    "df['year'] = pd.DatetimeIndex(df['published_date']).year\n",
    "df['day'] = pd.DatetimeIndex(df['published_date']).day\n",
    "df['week_day']= df['published_date'].apply(lambda x: day_order[datetime.date(x.year, x.month, x.day).weekday()])\n",
    "\n",
    "# Adding a daily views column\n",
    "df['views_received_daily'] = df['views'] / ( df['total_days_since_published'] + 1 )\n",
    "\n",
    "df['speaker_1_average_views'] = df['speaker_1'].map(list(df.groupby('speaker_1').agg({'views_received_daily' : 'mean'}).sort_values(['views_received_daily'],ascending=False).to_dict().values())[0])\n",
    "\n",
    "df['event_average_views'] = df['event'].map(list(df.groupby('event').agg({'views_received_daily' : 'mean'}).sort_values(['views_received_daily'],ascending=False).to_dict().values())[0])\n",
    "\n",
    "df.reset_index(drop=True, inplace = True)\n",
    "\n",
    "#Checking for unique topic\n",
    "unique_topics=[]\n",
    "for i in range(0,len(df)):\n",
    "  temp=df['topics'][i]\n",
    "  for i in temp:\n",
    "    if(i not in unique_topics):\n",
    "      unique_topics.append(i)\n",
    "      \n",
    "len(unique_topics)\n",
    "\n",
    "unique_topics_avg_view_dict={}\n",
    "for topic in unique_topics:\n",
    "  temp, count = 0, 0\n",
    "\n",
    "  for i in range(0,len(df)):\n",
    "    temp2=df['topics'][i]\n",
    "    if(topic in temp2):\n",
    "      temp+=df['views_received_daily'][i]\n",
    "      count+=1\n",
    "  unique_topics_avg_view_dict[topic]=temp//count\n",
    "\n",
    "topics_wise_average_views=[]\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "  temp=0\n",
    "  temp_topic=df['topics'][i]\n",
    "  for element in temp_topic:\n",
    "    temp+= unique_topics_avg_view_dict[element]\n",
    "  \n",
    "  topics_wise_average_views.append(temp//len(temp_topic))\n",
    "\n",
    "\n",
    "df['topics_wise_avg_views'] = pd.Series(topics_wise_average_views).values\n",
    "\n",
    "# Removing outliers\n",
    "df = df[((df['duration'] - df['duration'].mean()) / df['duration'].std()).abs() < 3]\n",
    "\n",
    "\n",
    "df['unique_topics'] = ((df['topics'].apply(ast.literal_eval)).apply(set)).apply(len)\n",
    "\n",
    "cleaneddf = df[['comments', 'duration', 'total_days_since_published', 'month', 'year', 'day',\n",
    "       'week_day', 'speaker_1_average_views',\n",
    "       'event_average_views', 'unique_topics', 'views_received_daily', 'transcript']]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3982 entries, 0 to 4003\n",
      "Data columns (total 12 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   comments                    3327 non-null   float64\n",
      " 1   duration                    3982 non-null   int64  \n",
      " 2   total_days_since_published  3982 non-null   int64  \n",
      " 3   month                       3982 non-null   object \n",
      " 4   year                        3982 non-null   int64  \n",
      " 5   day                         3982 non-null   int64  \n",
      " 6   week_day                    3982 non-null   object \n",
      " 7   speaker_1_average_views     3982 non-null   float64\n",
      " 8   event_average_views         3982 non-null   float64\n",
      " 9   unique_topics               3982 non-null   int64  \n",
      " 10  views_received_daily        3982 non-null   float64\n",
      " 11  transcript                  3982 non-null   object \n",
      "dtypes: float64(4), int64(5), object(3)\n",
      "memory usage: 404.4+ KB\n"
     ]
    }
   ],
   "source": [
    "cleaneddf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaneddf = cleaneddf[((cleaneddf['comments'] - cleaneddf['comments'].mean()) / cleaneddf['comments'].std()).abs() < 3]\n",
    "cleaneddf = cleaneddf[((cleaneddf['speaker_1_average_views'] - cleaneddf['speaker_1_average_views'].mean()) / cleaneddf['speaker_1_average_views'].std()).abs() < 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_w2v = r\"C:\\Users\\ABC\\Downloads\\GoogleNews-vectors-negative300.bin\"\n",
    "w2v_model = KeyedVectors.load_word2vec_format(pretrain_w2v, binary=True)\n",
    "\n",
    "\n",
    "def preprocess_corpus(texts):\n",
    "    mystopwords = set(stopwords.words(\"english\"))\n",
    "    def remove_stops_digits(tokens):\n",
    "        #Nested function that lowercases, removes stopwords and digits from a list of tokens\n",
    "        return [token.lower() for token in tokens if token.lower() not in mystopwords and not token.isdigit()\n",
    "               and token not in punctuation]\n",
    "    #This return statement below uses the above function to process twitter tokenizer output further. \n",
    "    return [remove_stops_digits(word_tokenize(text)) for text in texts]\n",
    "\n",
    "texts_processed = preprocess_corpus(cleaneddf['transcript'])\n",
    "def embedding_feats(list_of_lists):\n",
    "    DIMENSION = 300\n",
    "    zero_vector = np.zeros(DIMENSION)\n",
    "    feats = []\n",
    "    for tokens in list_of_lists:\n",
    "        feat_for_this =  np.zeros(DIMENSION)\n",
    "        count_for_this = 0 + 1e-5 # to avoid divide-by-zero \n",
    "        for token in tokens:\n",
    "            if token in w2v_model:\n",
    "                feat_for_this += w2v_model[token]\n",
    "                count_for_this +=1\n",
    "        if(count_for_this!=0):\n",
    "            feats.append(feat_for_this/count_for_this) \n",
    "        else:\n",
    "            feats.append(zero_vector)\n",
    "    return feats\n",
    "\n",
    "train_vectors = embedding_feats(texts_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame(train_vectors).join(cleaneddf.drop('transcript', axis =1))\n",
    "final = final.drop((final[final['views_received_daily'].isna()]).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in final.columns:\n",
    "    if final[i].isna().sum() > 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=final['views_received_daily']\n",
    "X=final.drop(columns=['views_received_daily'])\n",
    "\n",
    "X=pd.get_dummies(X)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "imputer = KNNImputer()\n",
    "imputer.fit(scaler.fit_transform(X))\n",
    "X = imputer.transform(scaler.fit_transform(X))\n",
    "X = scaler.inverse_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X).to_csv(\"X.csv\")\n",
    "pd.DataFrame(y).to_csv(\"y.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.999988060375447, 0.8657967308092372)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model= xgb.XGBRegressor(objective=\"reg:squarederror\")\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y,  test_size= 0.20, random_state= 5)\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "xgb_r2_train=xgb_model.score(X_train, y_train)\n",
    "xgb_r2_test= xgb_model.score(X_test, y_test)\n",
    "\n",
    "xgb_r2_train,xgb_r2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7575826194857351, 0.803618691474815, 0.7572741784132224, 0.8102358498672544)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "rr = Ridge(alpha=0.01) \n",
    "# higher the alpha value, more restriction on the coefficients; low alpha > more generalization,\n",
    "# in this case linear and ridge regression resembles\n",
    "rr.fit(X_train, y_train)\n",
    "rr100 = Ridge(alpha=100) #  comparison with alpha value\n",
    "rr100.fit(X_train, y_train)\n",
    "train_score=lr.score(X_train, y_train)\n",
    "test_score=lr.score(X_test, y_test)\n",
    "Ridge_train_score = rr.score(X_train,y_train)\n",
    "Ridge_test_score = rr.score(X_test, y_test)\n",
    "train_score, test_score, Ridge_train_score, Ridge_test_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
